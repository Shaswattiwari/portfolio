import streamlit as st 
def gram():
                st.header("""Building a Telegram Chatbot with Transformer-based Language Model""")
                st.markdown("In this blog post, we dive into the fascinating world of natural language processing and chatbots by exploring the development of a Telegram chatbot powered by a state-of-the-art transformer-based language model. Leveraging the capabilities of the Hugging Face transformers library, we demonstrate how to build a responsive and engaging chatbot capable of generating contextually relevant responses in real-time.")
                st.subheader("Introduction to Transformer-based Language Models")
                st.markdown("Transformer-based language models have revolutionized natural language processing tasks by enabling models to capture long-range dependencies and contextual information effectively. In our project, we utilize the Gemma model, a cutting-edge transformer architecture trained on a vast corpus of Italian text, to generate conversational responses.")                                  
                st.header("Setting Up the Environment")
                st.markdown("""Before diving into the code implementation, we need to set up our environment by installing the necessary dependencies, including the transformers library and the telebot package for interacting with the Telegram API. Additionally, we configure the Gemma model and tokenizer for Italian language processing.

                            """)
                st.code(""" #installing libs
                            !pip install -q -U accelerate==0.27.1
                            !pip install -q -U transformers==4.38.0
                            !pip install gemma
                            !pip install telebot
                            !pip install SpeechRecognition
                            !pip install pyTelegramBotAPI pydub
                            !pip install tesseract-ocr
                            !pip install pillow pytesseract
                            
                            #importing libs
                            import transformers
                            from transformers import AutoTokenizer, AutoModelForCausalLM
                            from transformers import GemmaTokenizer
                            from transformers import AutoTokenizer, pipeline
                            import torch
                            import telebot
                            import speech_recognition as sr
                            import os
                            import io
                            from pydub import AudioSegment
                            from PIL import Image, ImageEnhance, ImageFilter
                            import pytesseract
                            from pytesseract import TesseractError
                            
                            #getting model
                            
                            model = "google/gemma-2b-it"
                            tokenizer = AutoTokenizer.from_pretrained(model,token='#your API key')

                            pipeline = pipeline(
                                "text-generation",  # Specify the task as text generation
                                model=model,  # Specify the pre-trained model
                                model_kwargs={"torch_dtype": torch.bfloat16},  # Additional model arguments
                                device="cuda",  # Specify the device (CUDA for GPU, "cpu" for CPU)
                                token=''  # Hugging Face API token
                            )
                            
                            """)
                
                st.header("Developing the Chatbot")
                st.markdown("""With the environment set up, we proceed to develop our Telegram chatbot. We define functions to handle incoming messages, generate responses using the Gemma model, and send the responses back to the user. The chatbot utilizes a continuous polling mechanism to listen for new messages from users and respond promptly with contextually relevant content generated by the Gemma language model.

""")
                st.code("""def generate_response(text):
                                messages = [
                                    {"role": "user", "content": f"{text}"},
                                ]
                                prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                                outputs = pipeline(
                                    prompt,
                                    max_new_tokens=256,
                                    do_sample=True,
                                    temperature=0.7,
                                    top_k=50,
                                    top_p=0.95
                                )
                                response = outputs[0]["generated_text"][len(prompt):]
                                return response

                            def audio2text(input_file):
                                try:
                                    # Load the audio file
                                    audio = AudioSegment.from_file(input_file)

                                    # Convert the audio to the desired format
                                    converted_audio = audio.export(format="wav")

                                    # Recognize speech from the converted audio
                                    with io.BytesIO() as byte_io:
                                        byte_io.write(converted_audio.read())
                                        byte_io.seek(0)

                                        with sr.AudioFile(byte_io) as source:
                                            r = sr.Recognizer()
                                            audio_data = r.record(source)
                                            text = r.recognize_google(audio_data)
                                            return text
                                except sr.UnknownValueError:
                                    return "Error"
                                    
                                    
                                    
                            def handwriting_ocr(image_path):
                                try:
                                    # Open the image file
                                    with Image.open(image_path) as img:
                                        # Enhance the image for better OCR results
                                        enhanced_img = ImageEnhance.Contrast(img).enhance(2.0)
                                        # Apply pre-processing (optional)
                                        preprocessed_img = enhanced_img.filter(ImageFilter.SHARPEN)
                                        pytesseract.pytesseract.tesseract_cmd = '/bin/tesseract'

                                        # Perform OCR
                                        text = pytesseract.image_to_string(preprocessed_img)
                                        return text
                                except TesseractError as e:
                                    return "Error"
                            """)                        
                                        
                st.header("Enhancing User Experience")
                st.markdown("""To enhance the user experience, we implement features such as greeting messages for new users and personalized responses based on the input text. By leveraging the power of transformer-based language models, our chatbot is capable of understanding and generating natural language responses, providing an interactive and engaging experience for users.""")
                st.code("""# Handler for the /start command
                            @bot.message_handler(commands=['start'])
                            def send_welcome(message):
                                bot.reply_to(message, "Welcome to YourBot! How can I assist you today?")

                            # Handler for all text messages
                            @bot.message_handler(func=lambda message: True)
                            def handle_message(message):

                                # Generate response using the model
                                response = generate_response(message.text)

                                # Reply with the model's response
                                bot.reply_to(message, response)

                            @bot.message_handler(content_types=['voice'])
                            def handle_voice_message(message):
                                try:
                                    # Download the voice message
                                    file_info = bot.get_file(message.voice.file_id)
                                    downloaded_file = bot.download_file(file_info.file_path)

                                    # Save the voice message locally
                                    with open("voice_message.ogg", 'wb') as new_file:
                                        new_file.write(downloaded_file)

                                    # Perform speech recognition on the saved audio file
                                    recognized_text = audio2text("voice_message.ogg")

                                    if recognized_text == "Error":
                                    bot.reply_to(message,'Sorry, could not understand the audio.')

                                    else:
                                    response = generate_response(recognized_text)
                                    bot.reply_to(message, response)

                                except Exception as e:
                                    print("Error:", e)
                                    bot.reply_to(message, "Sorry, I could not understand the audio.")


                            @bot.message_handler(content_types=['photo'])
                            def handle_img_message(message):
                                try:

                                    _caption=message.caption
                                    # Get the photo file ID
                                    file_id = message.photo[-1].file_id

                                    # Download the photo
                                    file_info = bot.get_file(file_id)
                                    downloaded_file = bot.download_file(file_info.file_path)

                                    # Save the photo locally
                                    with open("img_text.jpg", 'wb') as new_file:
                                        new_file.write(downloaded_file)

                                    img_text=handwriting_ocr("img_text.jpg")
                                    if img_text == "Error":
                                    bot.reply_to(message,'Sorry, could not understand.')

                                    else:
                                    if _caption is not None:
                                        img_text=img_text+' '+_caption
                                        response = generate_response(img_text)
                                        bot.reply_to(message, response)
                                    else:
                                        response = generate_response(img_text)
                                        bot.reply_to(message, response)



                                except Exception as e:
                                print("Error:", e)



                            # Start the bot
                            bot.polling()""")
                
                st.header("Conclusion")
                st.markdown("""In conclusion, we have demonstrated the development of a Telegram chatbot powered by a transformer-based language model. By leveraging state-of-the-art natural language processing techniques, our chatbot is capable of engaging users in meaningful conversations and providing contextually relevant responses in real-time. As transformer-based models continue to advance, the possibilities for building intelligent and responsive chatbots are limitless, opening up exciting opportunities for natural language understanding and interaction.""")